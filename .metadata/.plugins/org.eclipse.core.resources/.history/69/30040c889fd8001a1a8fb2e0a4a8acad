package com.sanjay.spark

import org.apache.spark._
import org.apache.spark.SparkContext._
import org.apache.log4j._
import org.apache.spark.sql.{DataFrame, Dataset, SparkSession}
import org.apache.spark.sql.functions._


/** Count up how many of each star rating exists in the MovieLens 100K data set. */
object RatingsCounter {
 
  /** Our main function where the action happens */
  def main(args: Array[String]) {
   
    // Set the log level to only print errors
    Logger.getLogger("org").setLevel(Level.ERROR)
        
    // Create a SparkContext using every core of the local machine, named RatingsCounter
    //val sc = new SparkContext("local[*]", "RatingsCounter")
   
   val conf = new SparkConf().setAppName("SOME APP NAME").setMaster("local[*]")

    val sc = new SparkContext(conf)
    val spark = SparkSession.builder().appName("Spark Hive Example").getOrCreate()
    
    val jdbcDF = spark.read
  .format("jdbc")
  .option("url", "jdbc:hive2://52.66.218.85:10000/FIRSTDB")
  .option("dbtable", "FIRSTTABLE ")
  .option("user", "hive ")
  .option("password", "qfbmD3ggd5LdTJ1z")
  //.option("driver", "org.apache.hadoop.hive.jdbc.HiveDriver")
  .load()

println("able to connect------------------")

val sqlDF = spark.sql("select * from FIRSTTABLE")
println("Start println-----")
spark.sqlContext.sql("select * from FIRSTDB.FIRSTTABLE").collect().foreach(println)
println("end println-----")
sqlDF.show(false)

  }
}
